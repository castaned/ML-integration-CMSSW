{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "editable": true,
    "papermill": {
     "duration": 14.069112,
     "end_time": "2023-12-08T11:23:24.841190",
     "exception": false,
     "start_time": "2023-12-08T11:23:10.772078",
     "status": "completed"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import tables\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, ProgressBar\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "np.random.seed(seed=1)\n",
    "L.seed_everything(seed=1, workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: A_Dr_Z, Shape: (400000,), Type: float32\n",
      "Dataset: A_Zmass, Shape: (400000,), Type: float32\n",
      "Dataset: B_Dr_Z, Shape: (400000,), Type: float32\n",
      "Dataset: B_Zmass, Shape: (400000,), Type: float32\n",
      "Dataset: C_Dr_Z, Shape: (400000,), Type: float32\n",
      "Dataset: C_Zmass, Shape: (400000,), Type: float32\n",
      "Dataset: D_Dr_Z, Shape: (400000,), Type: float32\n",
      "Dataset: D_Zmass, Shape: (400000,), Type: float32\n",
      "Dataset: Dataset_ID, Shape: (400000,), Type: float32\n",
      "Dataset: Electron_charge, Shape: (400000, 10), Type: float32\n",
      "Dataset: Electron_cutBased, Shape: (400000, 10), Type: float32\n",
      "Dataset: Electron_eta, Shape: (400000, 10), Type: float32\n",
      "Dataset: Electron_phi, Shape: (400000, 10), Type: float32\n",
      "Dataset: Electron_pt, Shape: (400000, 10), Type: float32\n",
      "Dataset: MET_pt, Shape: (400000,), Type: float32\n",
      "Dataset: Muon_charge, Shape: (400000, 10), Type: float32\n",
      "Dataset: Muon_eta, Shape: (400000, 10), Type: float32\n",
      "Dataset: Muon_highPtId, Shape: (400000, 10), Type: float32\n",
      "Dataset: Muon_isGlobal, Shape: (400000, 10), Type: float32\n",
      "Dataset: Muon_phi, Shape: (400000, 10), Type: float32\n",
      "Dataset: Muon_pt, Shape: (400000, 10), Type: float32\n",
      "Dataset: event, Shape: (400000,), Type: float32\n",
      "Dataset: nElectron, Shape: (400000,), Type: float32\n",
      "Dataset: nMuon, Shape: (400000,), Type: float32\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(\"/eos/user/c/castaned/ntuple_merged_25.h5\", \"r\") as f:\n",
    "    for key in f.keys():\n",
    "        print(f\"Dataset: {key}, Shape: {f[key].shape}, Type: {f[key].dtype}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the list of variables from the h5 file to use as features, spectator and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27 features\n",
    "features = ['A_Dr_Z',\n",
    "            'A_Zmass',\n",
    "            'MET_pt',\n",
    "            'B_Zmass',\n",
    "            'B_Dr_Z'\n",
    "           ]\n",
    "\n",
    "# spectators to define mass/pT window\n",
    "# spectators = ['fj_sdmass',\n",
    "#               'fj_pt']\n",
    "\n",
    "#\n",
    "labels = ['Dataset_ID']\n",
    "\n",
    "nfeatures = len(features)\n",
    "# nspectators = len(spectators)\n",
    "# nlabels = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset_ID']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_labels(file_name):\n",
    "   \n",
    "    h5file = tables.open_file(file_name, 'r')\n",
    "\n",
    "    nevents = getattr(h5file.root, features[0]).shape[0]  # number of events for all data\n",
    "\n",
    "    feature_array = np.zeros((nevents, nfeatures))  # Cada feature tiene 10 valores por evento\n",
    "    label_array = np.zeros((nevents,2))\n",
    "    \n",
    "     # load feature arrays\n",
    "    for (i, feat) in enumerate(features):\n",
    "        feature_array[:,i] = getattr(h5file.root,feat)[:]\n",
    "\n",
    "    # Define which Dataset_ID values correspond to background (QCD)\n",
    "    bkg_ids = {2, 3, 4,5}  # Modify with actual background IDs\n",
    "\n",
    "    # Load labels arrays\n",
    "    for (i, label) in enumerate(labels):\n",
    "        dataset_id_array = getattr(h5file.root, label)[:]  # Load Dataset_ID from HDF5\n",
    "        label_array[:, 0] = np.isin(dataset_id_array, list(bkg_ids)).astype(int)  # Background\n",
    "        label_array[:, 1] = 1 - label_array[:, 0]  # Signal\n",
    "    \n",
    "    h5file.close()\n",
    "    return feature_array, label_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 5)\n",
      "(400000, 2)\n"
     ]
    }
   ],
   "source": [
    "# load training file\n",
    "feature_array, label_array = get_features_labels('/eos/user/c/castaned/ntuple_merged_25.h5')\n",
    "print(feature_array.shape)\n",
    "print(label_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide train and validation+test\n",
    "X_train, X_temp = train_test_split(feature_array, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# Divide validation+test in validation and test\n",
    "X_valid, X_test = train_test_split(X_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data (numpy.array) into a DatasetLoad (Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset: np.array):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.FloatTensor(self.dataset[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "#### AE: Pytorch Lightning Model Implementation\n",
    "Implementation of a plain Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_train_ds = SensorDataset(X_train)\n",
    "ae_valid_ds = SensorDataset(X_valid)\n",
    "ae_test_ds = SensorDataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0.0000,   0.0000, 120.6265,   0.0000,   0.0000])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ae_train_ds[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(L.LightningModule):\n",
    "\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 16),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(64, in_dim)\n",
    "        )\n",
    "\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input = batch\n",
    "        output = self.forward(input)\n",
    "        loss = F.smooth_l1_loss(output, input)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self.training_losses.append(self.trainer.callback_metrics['train_loss'].item())\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input = batch\n",
    "        output = self.forward(input)\n",
    "        loss = F.smooth_l1_loss(output, input)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.validation_losses.append(self.trainer.callback_metrics['val_loss'].item())\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=None):\n",
    "        input = batch\n",
    "        output = self.forward(input)\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Progress Bar Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleProgressBar(ProgressBar):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bar = None\n",
    "        self.enabled = True\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if self.enabled:\n",
    "            self.bar = tqdm(total=self.total_train_batches,\n",
    "                            desc=f\"Epoch {trainer.current_epoch+1}\",\n",
    "                            position=0,\n",
    "                            leave=True)\n",
    "            self.running_loss = 0.0\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if self.bar:\n",
    "            self.running_loss += outputs['loss'].item()\n",
    "            self.bar.update(1)\n",
    "            loss = self.running_loss / self.total_train_batches\n",
    "            self.bar.set_postfix(loss=f'{loss:.4f}')\n",
    "            # self.bar.set_postfix(self.get_metrics(trainer, pl_module))\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module) -> None:\n",
    "        if self.bar:\n",
    "            val_loss = trainer.logged_metrics['val_loss'].item()\n",
    "            loss = self.running_loss / self.total_train_batches\n",
    "            self.bar.set_postfix(loss=f'{loss:.4f}', val_loss=f'{val_loss:.4f}')\n",
    "            self.bar.close()\n",
    "            self.bar = None\n",
    "\n",
    "    def disable(self):\n",
    "        self.bar = None\n",
    "        self.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = ae_train_ds.dataset.shape[1]\n",
    "model = Autoencoder(in_dim)\n",
    "\n",
    "batch_size = 32\n",
    "ae_tdl, ae_vdl = DataLoader(ae_train_ds, batch_size=batch_size, num_workers=4), DataLoader(ae_valid_ds, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/eos/user/c/castaned/.local/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /eos/home-i04/c/castaned/VAE_training/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name    | Type       | Params | Mode \n",
      "-----------------------------------------------\n",
      "0 | encoder | Sequential | 3.2 K  | train\n",
      "1 | decoder | Sequential | 3.2 K  | train\n",
      "-----------------------------------------------\n",
      "6.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 K     Total params\n",
      "0.025     Total estimated model params size (MB)\n",
      "16        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Epoch 1:   3%|▎         | 333/10000 [00:27<14:36, 11.03it/s, loss=1.4180]"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", min_delta=0.00001, patience=5)\n",
    "cp = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\", mode=\"min\")\n",
    "pb = SimpleProgressBar()\n",
    "trainer = L.Trainer(callbacks=[pb, es, cp], max_epochs=1_000, logger=False, enable_checkpointing=True, accelerator=\"auto\")\n",
    "#trainer.fit(model, train_dataloaders=ae_tdl)\n",
    "trainer.fit(model, train_dataloaders=ae_tdl, val_dataloaders=ae_vdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = len(model.training_losses)\n",
    "x = np.linspace(1, num_iter, num_iter)\n",
    "fig = plt.figure()\n",
    "plt.plot(x, model.training_losses, label=\"training_loss\")\n",
    "plt.plot(x, model.validation_losses[1:], label=\"validation_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cp.best_model_path)\n",
    "print(cp.best_model_score)\n",
    "model = Autoencoder.load_from_checkpoint(cp.best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AE Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = SensorDataset(X_test)\n",
    "tdl = DataLoader(test_ds, batch_size=batch_size, num_workers=3)\n",
    "pred = trainer.predict(model, tdl)\n",
    "\n",
    "try:\n",
    "    reco = np.mean((np.vstack(pred) - X_test) ** 2, axis=1)\n",
    "except ValueError:  # caused by VAE\n",
    "    pred = [tup[0] for tup in pred]\n",
    "    reco = np.mean((np.vstack(pred) - X_test) ** 2, axis=1)\n",
    "    \n",
    "# Definir el umbral \n",
    "#threshold = np.median(reco) * 5 \n",
    "\n",
    "# Identificar los puntos anómalos\n",
    "#test_label_colors = np.where(reco > threshold, 'red', 'blue')  \n",
    "\n",
    "label_color_mapping = {0: 'blue', 1: 'red'}\n",
    "test_label_colors = np.array([label_color_mapping[label] for label in y_test[:, 1]])\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=label)\n",
    "                   for label, color in label_color_mapping.items()]\n",
    "\n",
    "# Overview\n",
    "plt.figure(figsize=(30, 4))\n",
    "scatter = plt.scatter(range(X_test.shape[0]), reco, label='Reconstruction Error', c=test_label_colors)\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error')\n",
    "plt.legend(handles=legend_elements, title='Labels', labels=['Normal', 'Outlier'])\n",
    "plt.show()\n",
    "\n",
    "# Zoomed\n",
    "plt.figure(figsize=(35, 6))\n",
    "scatter_zoomed = plt.scatter(range(X_test.shape[0]), reco, label='Reconstruction Error', c=test_label_colors)\n",
    "plt.ylim(0, np.median(reco) * 15)\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Reconstruction Error')\n",
    "plt.legend(handles=legend_elements, title='Labels', labels=['Normal', 'Outlier'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
