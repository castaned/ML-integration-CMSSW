# Setting configuration

Before starting, you need to create the environment and clone the GitHub repository. If you already have all your data prepared, simply clone the repository and proceed to [training section](training/01_intro.md), where you will find two options: running the training in LXPLUS and outside LXPLUS.

### Log into LXPLUS server (CERN computers)

```bash
ssh username@lxplus.cern.ch
```

### Set up the required CMSSW version

```bash
cmsrel CMSSW_13_3_0
cmsenv
cd CMSSW_13_3_0/src
```

You will see many of directories inside `CMSSW_13_3_0/`, but you only need to work inside `src/` directory.

!!! warning "IMPORTANT"
    1. `cmsenv` needs to be executed every time you open a new terminal to activate the environment variables. You need to be inside `CMSSW_13_3_0/` directory.
    2. The [Data processing section](data_processing/01_set_config.md) has been tested only with the CMSSW_13_3_0 release. The training section is not affected.

### Clone the repository and compile

```bash
git clone https://github.com/castaned/ML-integration-CMSSW DeepNTuples
scram b -j N
```

!!! warning "IMPORTANT"
    Make sure not to change the name `DeepNTuples` for your local directory containing the GitHub repository, otherwise, it will not work.

In the second command, `N` represents the CPUs to use for compiling and building the code inside `src/` directory, which uses dependencies from the CMSSW system. If you are unsure about `N`, simply run `scram b` without the `-j` flag, it will use all the available CPUs.


!!! note "Resources"
    If you want to learn more about the CMSSW system, its structure, and commands, you can explore [Intro to CMSSW](https://cms-opendata-workshop.github.io/workshop2022-lesson-cmssw/) and [CMSSW SCRAM](https://twiki.cern.ch/twiki/bin/view/CMSPublic/SWGuideBuildFile).
    
### Set up GRID proxy for accessing files

Ensure you have a valid GRID certificate. This is neccesary to have acces to the GRID, the distributed system used to retrieve the CERN data. If you donâ€™t have a GRID certificate, follow the instructions [here](https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookStartingGrid#ObtainingCert). You need to generate the certificate and export it to the `~/.globus` directory in your LXPLUS user space, which is the standard location.

This certificate cannot be used directly, you need to use a proxy. The proxy is a temporary credential derived from your certificate. Now, generate the proxy (it will ask your GRID passphrase)

```bash
voms-proxy-init --voms cms --valid 192:00 --out $HOME/.globus/x509up_u$(id -u)
```

This command created the proxy to be used with CMS data and sets its lifitime to 192 hours, if you do not specify this, it defaults to 12 hours, and 192 hours is the maximum allowed.

We store the proxy inside our home directory because it is safer and easier to use when submitting jobs. If you do not specify the output, it is stored by default in the `/tmp/` directory. The `x509up_u$(id -u)` format is a naming convention that includes your user ID.

!!! warning "Important"
    When the time expires, you must execute the command again to renew the proxy.

Now, verify that the certificate was correctly generated. First, we need to tell the system where our proxy is located, as mentioned earlier, the default directory is `/tmp/`, and the system will not find it if we saved it elsewhere. Therefore, we export the actual location

```bash
export X509_USER_PROXY=$HOME/.globus/x509up_u$(id -u)
```

The next command shows us the proxy information

```bash
voms-proxy-info --all
```

### NanoAOD datasets and branch selections

CERN has a complex data retrieval system composed of different softwares and data centers. In general, people from the experiments, *e.g.* CMS colaborators, produce data format releases that contain information recorded by the detectors or generated by Monte Carlo (MC) simulations. These releases can be in formats such as RECO, AOD, NanoAOD, etc. The format of interest for us is **NanoAOD**.

These releases are datasets. Each dataset comes from an LHC run and contains different information about the recorded or simulated events. Multiples copies of these datasets exist across different data centers in the GRID. You can find more information about the nanoAOD releases in the GitLab project [cms-nanoAOD](https://gitlab.cern.ch/cms-nanoAOD).

!!! info
    To access the gitlab repository, you must log in with your CERN account.

To locate the datasets you need, you use the **Data Aggregation System (DAS)**, it is a catalog that helps find the exact Logical File Names (LFNs) of the samples. DAS has its own query language, and the dataset paths follow a defined structure. You can find more information about DAS in [Locating Data Samples](https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookLocatingDataSamples).

Once you identify the actual LFN of your desired samples, you use an entry point (or redirector) in **XRootD** to reach the endpoint, the nearest datacenter from your location that hosts the samples. The data is not downloaded; instead, it is read efficiently through the XRootD protocol and accessed directly from the LXPLUS cluster. For more details, see [Using Xrootd Service (AAA) for Remote Data Access](https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookXrootdService). 

To use the tool, you must create a text file with the following format

!!! example "Dataset path format file"
    /Some/Das/query1/NANOAOD1.root:ID_1
    
    /Some/Das/query1/NANOAOD2.root:ID_1
    
    ...
    
    /Some/Das/query2/NANOAOD1.root:ID_2
    
    /Some/Das/query2/NANOAOD2.root:ID_2
    
    ...
    
    /Some/Das/queryn/NANOAODn.root:ID_n

Esch ROOT file is associated with an ID. Root files from the same data release share the same ID. This ID will be the target variable in the model used to classify the physical phenomenon.

!!! warning "Important"
    The IDs need to be natural numbers.

If you need help identifying your samples' FLN, refer again to [Locating Data Samples](https://twiki.cern.ch/twiki/bin/view/CMSPublic/WorkBookLocatingDataSamples). If you already know the sample FLN and want to use all root files in it, you can use the script `map_DASquery.py` as follow 

```bash
map_DASquery.py /Some/Das/query/NANOAOD1 ... /Some/Das/query/NANOAOD2 datasets_FLN_filename.txt
```
The file is located in `DeepNTuples/MyNanoAODTools` directory.

