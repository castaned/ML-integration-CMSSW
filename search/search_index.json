{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CMS Machine Learning Framework Documentation","text":"<p>Welcome to a comprehensive documentation for CMS Machine Learning Framework,  an end-to-end pipeline designed to transform CMS experimental data into trained machine learning (ML) models for high-energy physics (HEP) analysis.</p> <p>This framework automates the full workflow inside CERN's destributed computing ecosystem, minimizing the amount of ML expertise required.</p>"},{"location":"#the-repository-can-be-found-in-ml-integration-cmssw","title":"The repository can be found in ML-integration-CMSSW","text":""},{"location":"#what-this-framework-provides","title":"What this Framework Provides","text":"<p>The framework enables physicists to:</p> <ul> <li>Process NanoAOD datasets from the CMS experiment using distributed computing</li> <li>Transform ROOT files into ML-ready HDF5 formats with custom filtering</li> <li>Train machine learning models with HEP data</li> <li>Deploy trained models for physics analysis inference</li> </ul> <p>All within CERN's computing infrastructure, with minimal prior ML experience required.</p>"},{"location":"#quick-start-path","title":"Quick Start Path","text":"<p>If you are new to CMS computing or ML, follow these steps:</p> <ol> <li>System Overview - Understand the complete workflow and the architecture of the framework.</li> <li>CERN Access Setup - Configure LXPLUS access, VOMS/GRID certificates, and environment.</li> <li>Dataset Processing - Process your NanoAOD files.</li> <li>ML Training - Train ML models using the prepared datasets.</li> </ol>"},{"location":"#critical-requirements","title":"Critical Requirements","text":"<p>Before You Begin</p> <p>You must have:</p> <ul> <li>Valid CERN computing account with LXPLUS access</li> <li>Basic familiarity with Linux command line</li> <li>Understanding of ROOT files</li> <li>Python knowledge</li> </ul>"},{"location":"#who-is-this-documentation-for","title":"Who is this documentation for?","text":"<ul> <li>CMS physicists performing ML-based analyses</li> <li>Students new to CMS computing</li> <li>Analysts needing a reproducible ML pipeline</li> <li>Anyone converting CMS datasets into ML-ready formats</li> </ul>"},{"location":"cern-systems/01_lxplus/","title":"LXPLUS","text":"<p>LXPLUS (Linux Public Login User Service) is the primary access point to the CERN computing cluster. This service shaped the login nodes used to interact with our files, directories, environment, and computational environment. from these login nodes, you submit the script to executed in the worker nodes where the actual processing occurs.</p> <p>As the name suggests, LXPLUS operates on Linux-based system, in particular a Red Hat Enterprise Linux distribution. To establish a conneciton to LXPLULS, you need:</p> <ul> <li>A valid CERN user account</li> <li>SSH (Secure shell) client software</li> </ul> <p>There are multiple SSH client options; for example, PuTTY, Bitvise, or the terminal either on either windows, mac, or linux. All methods require your CERN username and authentication credentials. More information in The LXPLUS Service. </p> <p>Upon connecting, You will be asked for your password and verification code. If you have not yet activated the two factor authentication (2FA) look up Setting up 2FA using your Smartphone. </p>"},{"location":"cern-systems/01_lxplus/#logging-into-lxplus","title":"Logging into LXPLUS","text":"<p>Establish a secure shell connection to the LXPLUS cluster using your CERN credentials. Exmaple using CLI (command li\\ ne interface):</p> <pre><code>ssh username@lxplus.cern.ch\n</code></pre> <p>Upon successful connection, you will be placed in your home directory.</p>"},{"location":"cern-systems/02_storage/","title":"Storage Systems","text":"<p>Upon successful log to LXPLUS, you are placed in your home directory within a distributed filesystem called AFS (Andrew File System), documentation in openAFS. Your AFS home directory follows this structure:</p> <p><code>/afs/cern.ch/user/&lt;initial&gt;/&lt;username&gt;</code></p> <p>Where <code>&lt;initial&gt;</code> is the first letter of your username and <code>&lt;username&gt; is your CERN login name. For example, if your</code>` is olopez, your home directory will be: <p><code>/afs/cern.ch/user/o/olopez</code></p> <p>This directory serves as your primary workspace for creating files, directories, and environement, submitting computational script for execution, etc.</p> <p>IMPORTANT</p> <p>In this directory we have two major constaints:</p> <ul> <li>The disk space quota in AFS is 10GB per home directory. While sufficient for code and small files, it is not possible to store CMS experimental data, which can rage from gigabytes to terabytes per dataset.</li> <li>It is not permitted to run CPU-intensive proccesses for extended periods of time. Otherwise, the administrator will terminate the process. </li> </ul> <p>To address AFS limitations, CERN provides the EOS storage system, a high-performace, petabyte-scale storage system optimized for large data files, where the qouta is significantlyl higher (e.g., 1 TB, though this may vary). Check EOS quick tutorial for beginners to learn more. The difference between AFS and EOS is beyond the scope, but we will explain why we use one or the other when it is necessary. </p> <p>Throughout this documentation, we are going to work in the AFS directory, the environment, files, scripts, will be located here, while all experimental data files will be saved in EOS.</p> <p>In addition to AFS and EOS, CERN relies heavily on CERNVM File System (CernVM-FS) for software distribution. CernVM-FS is a read-only, distributed filesystem designed to efficiently deliver experiment software, libraries, and runtime environments across the worldwide computing infrastructure. CMSSW releases, external dependencies, and experiment-wide software stacks are distributed via CVMFS. This allows users to access identical software environments on LXPLUS, worker nodes, and GRID sites without installing or maintaining local copies.</p> <p>In practice, this means that while your code and configuration live in AFS, your large datasets reside in EOS, and the CMS software stack is transparently provided through CVMFS. Together, these three systems form the backbone of the CMS computing environment, enabling scalable, reproducible, and efficient data processing across the CERN ecosystem. More detailed information can be found in CernVM-FS documentation.</p>"},{"location":"cern-systems/03_computing-resources/","title":"Computational resources","text":"<p>As I mentioned, LXPLUS is just a cluster of login nodes, the CPU intense calculations has to be done by the worker nodes. CERN provides several mechanisms to access powerful compute nodes:</p> <ul> <li> <p>SWAN offers an interactive data analysis environment, with jupyter notebook interface, pre-configured with ROOT, Python, and other scientific computing tools. It is ideal for exploratory analysis, prototyping, and visualization.</p> </li> <li> <p>SLURM (Simple Linux Utility for Resource Management) is a workload manager designed for multi-node, high-performance computing (HPC) environments. It excels at tightly-coupled parallel applications that span multiple servers.</p> </li> <li> <p>HTCondor is a distributed High-Throughput Computing (HTC) system that matches user job<sup>1</sup> requirements with available computational resources. The user requiere an amount of cpu, memory, disk, amongs other options, and HTCondor creates a virtual execution environment with the requiered specifications by aggregating resources from all the available computer servers across the CERN cluster. </p> </li> </ul> <p>CERN documentation states that SLURM is dedicated to running multi-nodes jobs (e.g. MPI programs), and that HTConodr should be used otherwise. In our case, no programs run in multi-node jobs.</p> <ol> <li> <p>In this context, a job refers to a user-defined computational task, or set of tasks, submitted to a cluster via a scheduler like SLURM or HTCondor.\u00a0\u21a9</p> </li> </ol>"},{"location":"cern-systems/04_CMSSW/","title":"CMSSW framework","text":"<p>While you could work with just LXPLUS, AFS, EOS, and HTCondor, you would lack access to CMS-specific software tools. The CMS collaboration has developed the software framework called CMSSW that encapsulates event reconstruction algorithms, calibration and correction tools, physics object definitions, analysis modules and utilities, amongs others. . The CMMSW framework is mantained in different versions, according to data-taking perioids, simulation campaigns, and software improvements. Each version contains specific configurations, corrections, and algorithms appropriate for particular analyses. The framework uses a modular architecture where users build their analysis code within the CMSSW structure.</p> <p>Normally, we set up the CMSSW in our AFS directory. It creates a directory with a defined structure where we can use modules like EDProducer, EDFilter, and EDAnalyzer (where user code is implemented); SCRAM (Source Configuration, Release, And Management), the CMS build program; Python-based configuration files that define data processing workflows; etc. For detailed CMSSW documentation, conslut: CMSSW Application Framework.</p> <p>While the machine learning training and inference components of this framework can oprate independently of CMSSW, the data processing stage requires a proper CMSSW environment to access CMS data formats and tools.</p> <p>These are the key CERN system components you need to know to follow this tutorial. There are more, some of which will be explained later when requiered. Now, let's begin with the data processing. </p>"},{"location":"cern-systems/05_GRID-authentication/","title":"Setting up GRID proxy for data access","text":"<p>CMS data is distributed across the Worldwide LHC Computing Grid (WLCG), a global network of computing centers. Accessing this data requires proper authenticaiton through a GRID certificate and proxy.</p> <p>A GRID certificate is a digital credential that identifies you to the GRID. If you don\u2019t have a GRID certificate, follow the instructions here. You need to generate the certificate and export it to the <code>~/.globus</code> directory in your LXPLUS user space, which is the standard location.</p> <p>The GRID certificate alone cannot be used directly for data access. You must generate a VOMS (Virtual Organization Membership Service) proxy. The proxy is a short-lived credential derived from your certificate, which includes yout CMS Virtual Organization (VO) membership information and has a limited lifetime for security.</p> <p>Execute the following command to generate a CMS specific proxy:</p> <pre><code>voms-proxy-init --voms cms --valid 192:00 --out $HOME/.globus/x509up_u$(id -u)\n</code></pre> <p>This command generates the proxy to be used with CMS data and sets its lifitime to 192 hours, if you do not specify this, it defaults to 12 hours, and 192 hours is the maximum allowed.</p> <p>We store the proxy inside our home directory. If you do not specify the output, it is stored by default in the <code>/tmp/</code> directory. The <code>x509up_u$(id -u)</code> format is a naming convention that includes your user ID.</p> <p>Important</p> <p>When the proxy expires, you must regenerate it using the same command. If not, it will result in authentication failures.</p> <p>Verify that the certificate was correctly generated. First, we need to tell the system where our proxy is located, as mentioned earlier, the default directory is <code>/tmp/</code>, and the system will not find it if we saved it elsewhere. Therefore, we export the actual location:</p> <pre><code>export X509_USER_PROXY=$HOME/.globus/x509up_u$(id -u)\n</code></pre> <p>This environment variable tells the data access tools where to find your autentication credentials. Add this line to your <code>~/.bashrc</code> file to make it permanent. The next command shows us if it is correctly configured and vaild:</p> <pre><code>voms-proxy-info --all\n</code></pre> <p>Check that the timeleft value shows sufficient remaining time before expiration.</p>"},{"location":"cern-systems/06-NanoAOD/","title":"NanoAOD format","text":"<p>Traditionally, the data flow in the CMS experiment is optimized for the selection, calibration, and reconstruction of physical events, following a well\u2011established processing chain. This chain includes the data acquisition process, illustrated in Figure 1, after which the data undergo a reconstruction and progressive reduction workflow that produces several data formats. Each format is designed for a specific level of analysis within the experiment.</p> <p>          Figure 1: Architecture of the CMS data acquisition (DAQ) system.     </p> <p>In the early stages, the RAW and RECO formats are produced. These formats contain low\u2011level information close to the original detector signals and the detailed reconstruction of physical objects. They are mainly used for calibration tasks, detector validation, and studies that require full access to machine\u2011level information.</p> <p>As the data move further along the processing chain, more compact and analysis\u2011oriented representations are generated, such as AOD, MiniAOD, and NanoAOD. These formats include higher\u2011level physical variables, such as reconstructed objects, identification flags, and derived quantities, and are optimized for statistical analyses and phenomenological studies.</p> <p>This hierarchical workflow, illustrated in Figure 2, strikes a balance between information richness and the need for computational and storage efficiency when analyzing large volumes of data.</p> <p>          Figure 2: CMS data formats.      </p> <p>At present, NanoAOD is the most compact and accessible data format available. In contrast to AOD and MiniAOD, which are larger and whose variables are stored as CMSSW\u2011specific object types, NanoAOD is a semi\u2011structured format composed of flat arrays.</p> <p>As a result, NanoAOD files can be read outside the CMS software environment, and the transition from this format to representations compatible with machine learning models is much more straightforward compared to other CMS data formats.</p>"},{"location":"cern-systems/06-NanoAOD/#root-files-and-ttree","title":"ROOT files and TTree","text":"<p>All these datasets are stored in the .root file format, provided by ROOT. ROOT is a data analysis framework created by Ren\u00e9 Brun and Fons Rademakers in 1995 at CERN, designed to efficiently store, access, process, visualize, and analyze very large volumes of data, a long standing challenge at CERN due to the massive amount of information produced in particle collisions.</p> <p>A ROOT file can contain several types of data structures. However, NanoAOD files consist exclusively of tree\u2011like structures, known as TTrees. These structures can be understood as tables stored in a columnar layout.</p> <p>The main TTree is called <code>Events</code>, which contains all the relevant particle\u2011level information used in physics analyses. Additional TTrees are auxiliary and, in most cases, include <code>LuminosityBlocks</code>, <code>Runs</code>, <code>MetaData</code>, and <code>ParameterSets</code>. However, they are not limit to just these TTrees.</p>"},{"location":"cern-systems/06-NanoAOD/#branch-structure-in-nanoaod","title":"Branch structure in NanoAOD","text":"<p>As shown in Figure 3, this type of data representation can be considered loosely structured and may become highly complex. Nevertheless, NanoAOD files are composed exclusively of branches, which can be interpreted as columns in a dataset.</p> <p>          Figure 3: TTree structure.     </p> <p>Each event (i.e., each dataset instance) can store either:</p> <ul> <li> <p>Scalar values</p> </li> <li> <p>One\u2011dimensional arrays of fixed length</p> </li> <li> <p>One\u2011dimensional arrays of variable length</p> </li> </ul> <p>The length of array\u2011type branches may vary from event to event. Each branch is associated with a specific data type, such as <code>bool</code>, <code>int32</code>, <code>uint64</code>, <code>float64</code>, and others, depending on the nature of the stored information.</p> <p>This simplified yet flexible structure is a key feature that makes NanoAOD particularly suitable for modern data analysis workflows and machine learning applications.</p>"},{"location":"cern-systems/07_dataset-discovery/","title":"Locating and understanding CMS datasets","text":""},{"location":"cern-systems/07_dataset-discovery/#data-formats","title":"Data formats","text":"<p>CERN has a complex data retrieval system composed of different softwares and data centers. In general, people of the experiments, e.g., CMS colaborators, produce data format releases that contain information recorded by the detectors from LHC runs or generated by Monte Carlo (MC) simulations. Each release corresponds to a specific data-taking period, trigger configuration, or simulation campaign. These releases can be in formats such as RAW, RECO, AOD, MiniAOD, and NanoAOD. In particular, NanoAOD is the newest format, released in 2016, which stores data using ROOT TTree objects instead of CMSSW C++ object like MiniAOD or AOD. Therefore, it can be analyzed using ROOT and/or python libraries.</p> <p>While the other formats contain very detailed information, they are also very large and cumbersone for analysis. NanoAOD retains the most commonly used information for analysis while dramatically reducing the data volume, and it is easier to work with becuase of its Ntuple-like format. Find more information about nanoAOD in Exploring CMS nanoAOD, The CMS NanoAOD data tier, and cms-nanoAOD.</p>"},{"location":"cern-systems/07_dataset-discovery/#using-das-to-locate-datasets","title":"Using DAS to locate datasets","text":"<p>The Data Aggregation System (DAS) is the catalog that helps find the exact Logical File Names (LFNs) of the samples. It provides a unified interface to query dataset locations, metedata, and file liests acrros the distributed GRID. DAS has its own query language, and the dataset paths follow a defined structure convention:</p> <p><code>/PrimaryDataset/ProcessedDataset/DataTier/</code></p> <p>An example of an LFN:</p> <ul> <li><code>/ZGToLLG_01J_5f_TuneCP5_13TeV-amcatnloFXFX-pythia8/RunIISummer20UL18NanoAODv9-106X_upgrade2018_realistic_v16_L1v1-v1/NANOAODSIM</code></li> </ul> <p>Where <code>ZGToLLG_01J_5f_TuneCP5_13TeV-amcatnloFXFX-pythia8</code> is the primary dataset, <code>RunIISummer20UL18NanoAODv9-106X_upgrade2018_realistic_v16_L1v1-v1</code> is the processed dataset, and <code>NANOAODSIM</code> is the data tier. There are two way to find the LFNs: using the web interface, or using the command line in a cluster that has accessed. Documentation on how to query the datasets can be found in Locating Data Samples.</p>"},{"location":"cern-systems/07_dataset-discovery/#xrootd-protocol-for-data-access","title":"XRootD protocol for data access","text":"<p>Once you identify the actual LFN of your desired samples, you use the XRootD protocol to access them. XRootD provides a redirector service (entry point) that automatically connects you to the optimal data center (end point) baseppd on network proximity and file availability. The data is not downloaded; instead, it is read efficiently through the XRootD protocol and accessed directly from the LXPLUS cluster. For more details, see Using Xrootd Service (AAA) for Remote Data Access.</p>"},{"location":"datasets/01_nanoaod/","title":"NanoAOD Format","text":"<p>The NanoAOD (Nano Analysis Object Data) format is a lightweight and analysis-friendly data format used in the CMS Experiment at CERN. It is designed to provide a compact representation of event-level information, keeping only the most essential quantities for physics analyses.</p>"},{"location":"datasets/01_nanoaod/#structure-overview","title":"Structure Overview","text":"<p>NanoAOD files are ROOT files containing a single <code>Events</code> tree with branches for reconstructed and generator-level objects. Each branch stores arrays (or collections) of physics objects per event.</p> <p>Typical groups of branches include:</p> <ul> <li>Event information: run, luminosity block, event ID, generator weight, etc.  </li> <li>Physics objects:</li> <li>Electrons (<code>Electron_pt</code>, <code>Electron_eta</code>, <code>Electron_phi</code>, \u2026)</li> <li>Muons (<code>Muon_pt</code>, <code>Muon_eta</code>, <code>Muon_iso</code>, \u2026)</li> <li>Jets (<code>Jet_pt</code>, <code>Jet_btag</code>, <code>Jet_mass</code>, \u2026)</li> <li>Missing transverse energy (MET) (<code>MET_pt</code>, <code>MET_phi</code>)</li> <li>Trigger and filter information: HLT paths, event flags, etc.</li> </ul>"},{"location":"datasets/01_nanoaod/#documentation-versioning","title":"\ud83d\udcda Documentation &amp; Versioning","text":"<p>The NanoAOD format evolves with each CMSSW release, and the detailed documentation \u2014 including branch structure, schema definitions, and version notes \u2014 is maintained in the official repositories below.</p> <p>\u26a0\ufe0f Access Warning The CERN GitLab wiki requires CMS credentials (an active CERN account with CMS project access). If you are not logged in, you may see a 404 or \"Access Denied\" page.</p> <ul> <li> <p>Official NanoAOD Documentation Wiki (CERN GitLab)   Detailed versioning information, variable definitions, and schema updates:   \ud83d\udd17 https://gitlab.cern.ch/cms-nanoAOD/nanoaod-doc/-/wikis/home</p> </li> <li> <p>NanoAOD-tools on GitHub   Post-processing utilities, friend tree management, and helper scripts for analysis:   \ud83d\udd17 https://github.com/cms-nanoAOD/nanoAOD-tools</p> </li> <li> <p>NanoAOD definitions in CMSSW   Source code for NanoAOD production within the CMSSW framework:   \ud83d\udd17 https://github.com/cms-sw/cmssw/tree/master/PhysicsTools/NanoAOD</p> </li> </ul>"},{"location":"framework/01_architecture/","title":"General architecture","text":"<p>The developed framework is organized into two main sections, which can be used independently depending on the user\u2019s needs. The first section focuses on processing data from CERN experiments, while the second section is dedicated to the training and usage of machine learning models. Each part is designed to be modular, reproducible, and configurable through YAML configuration files, facilitating adoption by a wide range of users.</p> <p>             Figure 1: Framework architecture and workflow.       </p>"},{"location":"framework/01_architecture/#processing-and-preparation-of-experimental-data","title":"Processing and preparation of experimental data","text":"<p>This section is responsible for accessing, processing, and converting official CMS experiment data stored on the GRID. All operations are carried out within the LXPLUS environment and using CMS tools, in particular CMSSW, which guarantees full compatibility with CMS data formats, conventions, and metadata.</p>"},{"location":"framework/01_architecture/#access-and-initial-processing-of-nanoaod","title":"Access and initial processing of NanoAOD","text":"<p>At this stage, NanoAOD files stored on the GRID are read using standard CMS mechanisms such as XRootD and DAS. Once the appropriate file endpoint is obtained, the framework automatically generates the required jobs to be executed on compute nodes dedicated to intensive processing.</p> <p>The system transfers the necessary execution environment to each node (scripts, configuration files, and the CMSSW environment) and processes events by applying user-defined filters, selection cuts, and transformations.</p> <p>The output of this phase consists of processed ROOT files collected on EOS, ready to be converted into formats more suitable for machine learning applications.</p>"},{"location":"framework/01_architecture/#conversion-from-root-to-hdf5","title":"Conversion from ROOT to HDF5","text":"<p>In this step, the framework generates a new set of jobs that take as input the previously produced ROOT files and convert them into the HDF5 format. HDF5 is a standard format in Python-based machine learning workflows and is widely used in the scientific community.</p> <p>Although NanoAOD variables are typically scalars or fixed-length arrays, some variables may contain arrays of variable length. In such cases, these arrays are converted into fixed-length representations. As illustrated in Figure 1, this stage represents the transition from a particle physics problem to a data science problem.</p> <p>Each stage of the workflow is driven by a YAML configuration file in which the user specifies input and output paths, variables to be processed, computational resources, environment parameters, and other relevant settings.</p> <p>While the ROOT-to-HDF5 conversion can, in principle, be performed independently of LXPLUS and CMSSW, the strong coupling to these environments in this section enables full automation of the process using a single YAML configuration file.</p>"},{"location":"framework/01_architecture/#machine-learning","title":"Machine learning","text":"<p>The second section of the framework is dedicated to the development of binary or multiclass classification models based on neural networks, with the goal of identifying the most suitable architecture for a given problem. This phase was designed with portability and modularity as core requirements, such that training does not depend on LXPLUS-specific tools or on CMSSW.</p> <p>Although the repository provides examples using HTCondor together with Apptainer on LXPLUS, the system is designed to run in any environment that provides Python, standard Python machine learning libraries (in particular PyTorch), and common scientific analysis tools such as NumPy and h5py.</p> <p>The machine learning section can be decomposed into the following steps:</p> <ol> <li> <p>Workflow configuration.</p> </li> <li> <p>Transform ans split the dataset into training, validation, and test sets.</p> </li> <li> <p>Hyperparameter optimization.</p> </li> <li> <p>Distributed execution (optional, when using a resource manager).</p> </li> <li> <p>Saving the best trained model in the standard PyTorch format.</p> </li> <li> <p>Generation of metrics and figures for performance evaluation.</p> </li> <li> <p>Visualization and tracking of trained models using MLflow.</p> </li> <li> <p>Model inference.</p> </li> </ol> <p>As in the previous section, the configuration is defined through a YAML file specifying input and output paths, target model performance, the number of models to be generated, whether the task is binary or multiclass classification, and other relevant parameters.</p> <p>If the user chooses to run training through a resource manager, an additional file (such as an HTCondor or SLURM submission script) must be provided. Otherwise, the machine learning workflow can be executed directly using Python.</p>"},{"location":"framework/02_data-processing/","title":"Data processing","text":"<p>             Figure 1: MLFlow experiments.         </p>"},{"location":"framework/03_ml/","title":"Machine learning","text":"<p>             Figure 1: MLFlow experiments.         </p>"},{"location":"framework/04_framework-setup/","title":"Framework setup","text":"<p>With data access credentials established, and data identified, you must now configure the CMSSW environment appropriate for your data, and clone the repository with the new framework.</p>"},{"location":"framework/04_framework-setup/#setting-up-the-cmssw-release","title":"Setting up the CMSSW release","text":"<p>Identify the correct CMSSW release for your data. This information is provided in dataset documentation. Execute these commands in sequence:</p> <pre><code>cmsrel CMSSW_13_3_0\ncd CMSSW_13_3_0/src\ncmsenv\n</code></pre> <p>The first command create a directory with all the files in the framework. You will see many of directories inside <code>CMSSW_13_3_0/</code>, each one has its own function, but we will work just inside <code>src/</code> directory.</p> <p>IMPORTANT</p> <ol> <li><code>cmsenv</code> needs to be executed every time you open a new terminal to activate the environment variables. You need to be inside <code>CMSSW_13_3_0/</code> directory.<ol> <li>The data processing component documented here has been tested only with the <code>CMSSW_13_3_0</code> release. The training section is not affected.</li> </ol> </li> </ol> <p>Resources</p> <p>If you want to learn more about the CMSSW system, its structure, and commands, you can explore Intro to CMSSW and CMSSW SCRAM.</p>"},{"location":"framework/04_framework-setup/#repository","title":"Repository","text":"<p>From within your CMSSW <code>src/</code> directory, clone the framework repository:</p> <pre><code>git clone https://github.com/castaned/ML-integration-CMSSW some_name\ncd some_name\n</code></pre> <p><code>some_name</code> is the directory name for the cloned repository. The repository contains two main directories:</p> <ul> <li> <p><code>data_processing/</code>: Tools for retrieving, filtering, and converting CMS data</p> </li> <li> <p><code>ml_training/</code>: Tools for training machine learning models and performing inference.</p> </li> <li> <p><code>example_files</code>: Example files used in the Example of usage section of this documentation.</p> </li> </ul> <p>These components are designed to function independently. If you work only requires data processing, you may safely remove the <code>ml_training/</code> directory.</p>"},{"location":"getting-started/01_overview/","title":"Overview and objectives","text":"<p>This repository provides a comprehensive framework for processing, training, and deploying machine learning models within the context of CMS (Compact Muon Solenoid) experiments at CERN (Conseil Europ\u00e9en pour la Recherche Nucl\u00e9aire, or European Council for Nuclear Research in english). The framework facilitates data preparation, model training, and evaluation to support ML-based analyses in high-energy physics.</p> <p>In this framework, the primary goal is to maximize accessibility for physisits who are not used to machine learning development, as well as for new users navigating the complaex CERN computing ecosystem, particularly LXPLUS (Linux Public Login User Service) and CMSSW (CMS SoftWare).</p> <p>CERN documentation can be difficult to navigate without knowing where to begin. There are multiple websites with different pieces of information, some of which are duplicated, with certain versions beings newer than others. Therefore, this documentation aims to explain the basic concepts and provide references to the pages where we found useful information for implementing and usuing the entire workflow.</p> <p>Info</p> <p>Some documentation requires authentication with CERN account to access.</p>"},{"location":"getting-started/02_architecture/","title":"System architecture and workflow","text":"<p>The framework is organized around a pipeline that transforms CMS experimental data into trained machine learning models The following diagram illustrates the complete data flow:</p> <p></p> <p>The workflow begins with datasets stored in the CMS data storage, particularly focusing on the NanoAOD data format, which consists of a Ntuple-like, analysis-ready format containing selected physics objects and event information. More information The CMS NanoAOD data tier.</p> <p>The process unfolds in the following stages:</p> <ul> <li> <p>Data retrieval and filtering: The first stage involves retrieving NanoAOD files from the distributed CMS data infrastructure (the GRID), applying user-defined filtering or skimming scripts to select relevant events and physics objects, and preparing the data for further processing.</p> </li> <li> <p>Format conversion: The filtered ROOT files (the native data format used in high-energy physics) are transformed into HDF5 files. The HDF5 format integrates seamlessly with modern Python-based machine learning frameworks such as TensorFlow, PyTorch, and scikit-learn. PyTorch is the framework used in the framework. This step is separated form the rest to allow for the transformation of ROOT files, even if they were not processed in the last stage.</p> </li> <li> <p>Model training: The HDF5 files serve as input for the training stage, where machine learning models are developed, different hyperparameter configurations are tested, and performance metrics are evaluated and visualized.</p> </li> <li> <p>Inference deployment: The final stage involves applying the trained model to new data to make predictions or classifications.</p> </li> </ul> <p>The dashed red square in the diagram specifically highlights the training part, which is designed to run in different computing environments. While this documentation primarily focuses on execution within the LXPLUS ecosystem, the machine learning module can be executed on any cluster, server, or local machine</p> <p>Throughout this documentation we work within the CMS ecosystem. The framework leverages CMSSW releases, which contain the necessary physics objects, algorithms, and packages requiered for the data processing part.</p> <p>Before diving into the practical implementation, let's understand the essencial infrastructure and software by providing a comprehensive introduction to the key systems you will interact with throughout this workflow.</p>"},{"location":"usage/01_usage/","title":"Complete workflow","text":"<p>This section demonstrates an end-to-end example of running the complete workflow, data processing, root into HDF5 conversion, and ML model training.</p> <p>We assume the following:</p> <ul> <li>You have a LXPLUS account and a GRID certificate.</li> <li>You already know the FLN of your datasets.</li> <li>You have set up your CMSSW environment and are inside the <code>src/</code> directory.</li> <li>You have your processing scripts for your specific analysis.</li> </ul> <p>First, clone the GitHub repository:</p> <pre><code>git clone https://github.com/castaned/ML-integration-CMSSW &lt;directory_name&gt;\ncd &lt;directory_name&gt;\n</code></pre> <p>You can choose any <code>&lt;directory_name&gt;</code> you like. In this example <code>&lt;directory_name&gt;</code> will be <code>ml_framework</code>.</p> <p>Info</p> <p>Remember that all example files can be found in the <code>example_files</code> directory of the repository.</p> <p>First, we processed and convert the data. We move to the data processing directory:</p> <pre><code>cd data_processing\n</code></pre> <p>Knowing the FLN, and having the processing script, we can process the data by modifying the YAML file <code>data_processing_config.yaml</code>:</p> <pre><code>---\nproxy:\n  generate: 1 # 1 yes, 0 no\n  voms: \"cms\"\n  proxy_time: \"192:00\"\n  proxy_path: \"$HOME/.globus/x509up_$(id -u)\"\n\ndata_processing:\n  condor_params:\n    executable_file: \"run_filter.sh\"\n    cpus: 1\n    gpus: 0\n    mem: 1.5GB\n    disk: 2GB\n    job_flavour: \"espresso\" # 20 minutes\n  processing_script: \"src/ml_framework/new_dataproc/example/filterNanoAOD.py\"\n  eos_output_dir: \"/eos/user/v/vminjare/test_dataprocessing\"\n  afs_cms_base: \"/afs/cern.ch/user/v/vminjare/CMSSW_13_3_0\"\n  redirector: \"cms-xrd-global.cern.ch\"\n  datasets:\n    - FLN: \"/ZGToLLG_01J_5f_TuneCP5_13TeV-amcatnloFXFX-pythia8/RunIISummer20UL18NanoAODv9-106X_upgrade2018_realistic_v16_L1v1-v1/NANOAODSIM\"\n      ID: 0\n      amount: 3 # -1 is all ROOT files\n    - FLN: \"/WprimeToWZToWlepZlep_narrow_M1000_TuneCP5_13TeV-madgraph-pythia8/RunIISummer20UL18NanoAODv9-106X_upgrade2018_realistic_v16_L1v1-v1/NANOAODSIM\"\n      ID: 1\n      amount: -1\n</code></pre> <p>Info</p> <p>Indentation in YAML files is very important, and YAML files do not accept tabs.</p> <p>The main processing script, <code>processing_script</code>, in this case <code>filterNanoAOD.py</code>, must satisfy the following conditions:</p> <ul> <li>It must accept exactly the following arguments input file path, FLN, OUTPUT DIRECTORY in this order.</li> <li>It must have a dataset labels branch taht start at 0 and increase sequentially without gaps, as the mapping declare in the <code>datasets</code> section in the YAML file.</li> </ul> <p>For more information, check the dataset processing section.</p> <p>If your GRID proxy certificate is still valid, do not generate a new one. Change <code>generate: 1</code> to <code>generate: 0</code> under the <code>proxy</code> section.</p> <p>In this example, I have two files to process the data:</p> <pre><code>example/\n\u251c\u2500\u2500 branchsel.txt\n\u2514\u2500\u2500 filterNanoAOD.py\n</code></pre> <p>In general terms, this analysis file selects events containing at least three leptons (electrons and/or muons), classifies the events into categories according to the lepton composition, and computes composite variables derived from the leptonic information.</p> <p>These variables include the reconstructed invariant mass of the three-lepton system, the total momentum, and other kinematic quantities that allow the definition of the region of interest in the search for specific exotic particles. This procedure is performed independently for each sample, LFN in the YAML file.</p> <p>Now we submit the jobs, one per ROOT file to process:</p> <pre><code>python3 execute_data_processing.py -f data_processing_config.yaml\n</code></pre> <p>When you execute the Python script, it first creates a tar archive of the entire <code>afs_cms_base</code> directory and uploads it to <code>eos_output_dir</code>. This step is performed locally in your current shell and may take several minutes. Once the jobs are submitted to HTCondor, you can monitor their status using <code>condor_q</code>. Also, the logs are stored in <code>logs/</code>. After all the jobs finish, the new ROOT file will be in the directory specified by <code>eos_output_dir</code>.</p> <p>Now we convert the ROOT file into HDF5 files. First, we go to the directory:</p> <pre><code>cd convert_h5\n</code></pre> <p>As before, configure the YAML file, for this part <code>root2h5_config.yaml</code>. For example:</p> <pre><code>---\nconvertion:\n  input_dirs:\n    - \"/eos/user/v/vminjare/test_dataprocessing/WprimeToWZToWlepZlep_narrow_M1000_TuneCP5_13TeV-madgraph-pythia8_RunIISummer20UL18NanoAODv9-106X_upgrade2018_realistic_v16_L1v1-v1_NANOAODSIM\"\n    - \"/eos/user/v/vminjare/test_dataprocessing/ZGToLLG_01J_5f_TuneCP5_13TeV-amcatnloFXFX-pythia8_RunIISummer20UL18NanoAODv9-106X_upgrade2018_realistic_v16_L1v1-v1_NANOAODSIM\"\n\n  tree_name: \"Events\"\n\n  branches: \"all\"\n\n# If you do not have jagged arrays leave it blank. If you use, the default value is 10\n  max_jagged_len:\n\n  eos_output_dir: \"/eos/user/v/vminjare/test_conversionh5\"\n\ncondor_params:\n  executable_file: \"run_conversion.sh\"\n  cpus: 1\n  gpus: 0\n  mem: 1.5GB\n  disk: 2GB\n  job_flavour: \"espresso\" # 20 minutes\n</code></pre> <p>For both conversion and training, a custom Python environment is required. We use containers for this. Before submitting the job, buil the container:</p> <pre><code>apptainer build conversion_container.sif conversion_container.def\n</code></pre> <p>This stage needs to run some code in the current shell, which requires Python libraries not available by default. These libraries are present in the CMSSW environment, so run <code>cmsenv</code> before submitting the jobs:</p> <pre><code>cmsenv\npython3 execute_convert_root2h5.py -f root2h5_config.yaml\n</code></pre> <p>When the jobs finish, your HDF5 files will be in <code>eos_output_dir</code>. If there is some problem, you can check the logs in the <code>logs/</code> directory.</p> <p>With the processed HDF5 data, we can create, optimize, and train our ML model. Go to the training directory:</p> <pre><code>cd ml_framework/ml_training\n</code></pre> <p>Training can be done with HTCondor, SLURM, our directly on any a server or compute.. In this example, wel run the training in LXPLUS, so we need to configure both a YAML file and a HTCondor submission file.</p> <p>Important</p> <p>Note that the <code>input_paths</code> variable in the YAML configuration file does not reference EOS paths (e.g. <code>/eos/user/v/vminjare/</code>), as this part of the framework is not tightly coupled to CERN-specific systems. Access to EOS is instead handled through the HTCondor submission file, which ensures that the appropriate environment and filesystem access are available at runtime.</p> <ul> <li><code>ml_model_config.yaml</code>:</li> </ul> <pre><code>---\ndata:\n  input_paths:\n    - \"test_conversionh5/WprimeToWZToWlepZlep_narrow_M1000_TuneCP5_13TeV-madgraph-pythia8_RunIISummer20UL18NanoAODv9-106X_upgrade2018_realistic_v16_L1v1-v1_NANOAODSIM\"\n    -  \"test_conversionh5/ZGToLLG_01J_5f_TuneCP5_13TeV-amcatnloFXFX-pythia8_RunIISummer20UL18NanoAODv9-106X_upgrade2018_realistic_v16_L1v1-v1_NANOAODSIM\"\n\n\n  output_path: \"results\"\n\n  features:\n    - \"A_Dr_Z\"\n    - \"A_Zmass\"\n    - \"MET_pt\"\n    - \"B_Zmass\"\n    - \"B_Dr_Z\"\n\n\n  label: \"Dataset_ID\"\n\n  num_classes: 2\n\n  # Leave this blank if you do not want to apply a mapping\n  label_mapping: \"short_name_mapping.json\"\n\nmodel:\n  name: \"mlp_binary_test\"\n\n  type: \"mlp\"\n\n  ideal_accuracy: 95\n\n  num_models: 3\n</code></pre> <ul> <li><code>lxplus_template.jdl</code></li> </ul> <pre><code># HTCondor Submission File\n\nenvironment = \\\n    EOS_INPUT_DIR=/eos/user/v/vminjare/test_conversionh5; \\\n    EOS_DIR=/eos/user/v/vminjare/outputs/$(ClusterId); \\\n    REDIRECTOR=root://eosuser.cern.ch; \\\n    RESULTS_DIR=results/\n\n#notify_user             = email@email.com\nnotification            = Complete\nuniverse                = vanilla\nexecutable              = lxplus_run_template.sh\nshould_transfer_files   = YES\nwhen_to_transfer_output = ON_EXIT\ntransfer_input_files    = ./\nx509userproxy = $ENV(X509_USER_PROXY)\noutput                  = job.$(ClusterId).$(ProcId).out\nerror                   = job.$(ClusterId).$(ProcId).err\nlog                     = job.$(ClusterId).$(ProcId).log\nrequest_cpus            = 5\nrequest_gpus            = 1\n+JobFlavour = \"microcentury\"\nqueue\n</code></pre> <p>Now build the container:</p> <pre><code>apptainer build ml_container.sif ml_container.def\n</code></pre> <p>This may take a few minutes. Once the <code>.sif</code> file is ready, export the proxy certificate to be able to have access to XRootD:</p> <pre><code>export X509_USER_PROXPROXY=/afs/cern.ch/user/v/vminjare/.globus/x509up_u170903\n</code></pre> <p>Finally, submit the job:</p> <pre><code>condor_submit lxplus_template.jdl\n</code></pre> <p>HTCondor logs will be created where we submitted the job. However, the scrpit generate its own logs files, stderr.log and stdout.log, they will be in <code>output_path/</code> along MLflow runs, plots, models files, and all result generated. If everythong went well, you will see the ROC curve and confusion matrix as follows:</p> <p>          Figure 3: ROC curve for the binary MLP classifier.     </p> <p>          Figure 4: Confusion matrix for the binary MLP classifier.     </p> <p>Finally, if you want to inspect the MLflow runs, copy <code>output_path/mlruns/</code> and the <code>ml_container.sif</code> to your computer and run:</p> <pre><code>apptainer exec ml_container.sif mlflow ui --backend-store-uri mlruns/ --host localhost --port 8100\n</code></pre> <p>The MLflow UI will then be available at <code>localhost:8100</code> in your web browser.</p> <p>          Figure 1: MLFlow experiments.     </p>"}]}